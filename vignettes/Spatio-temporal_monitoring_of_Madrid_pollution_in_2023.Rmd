---
title: "Spatio-temporal monitoring of Madrid pollution in 2023"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Spatio-temporal monitoring of Madrid pollution in 2023}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
)
```

```{r setup}
library(widepollution)  #load our package
library(dplyr)  #used to arrange dataframe rows
library(tidyr)  #used to pivot a dataframe to create a summary table
library(sf)     #used for spatial analysis throughout the markdown
library(units)  #units of MB
library(rjson)  #read json in processing stage
library(stars)  #used for data cube analysis
library(gstat)  #used for interpolation
```

# Spatio-temporal monitoring of Madrid pollution in 2023

Author: Rodrigo Malagón Rodríguez

# I. Introduction

Madrid suffers from high trends of pollution due to the strong use of cars and narrow crowded streets. Road traffic can drive as much as 90% of NO2 concentration in the centre of the city (Borge et al., 2014). Similar is the case of PM2.5, mainly caused by heavy car use in the city (Ortiz et al., 2017). Moreover, in previous years, $NO_2$ concentrations in the city surpassed the annual limit of 40 $\mu g/ m^3$, set by the EU (EEA, 2019).

Following this key facts, in this project we investigate which neighborhoods are mainly affected by pollution and how does this figure vary across time. To focus our work, we perform the analysis of $NO_x$ pollution. We perform spatial interpolation of pollution point data across time periods, we create datacubes and perform analyses on this useful structure.

The current vignette showcases the employment of processing functions from our package to aid in the construction of a useful pollution dataset of Madrid for the year 2023.

# II. Data processing

In this section we process our main datasets from [Madrid Open Data portal](https://datos.madrid.es/portal/site/egob). The main pollution datasets consis of **wide-format** monthly tables with hours on columns and measuring stations by type of pollutant by day on rows. We store all retrieved raw files for this project as **external data**. 

## II.0 On `NOTE` arising in `R CMD check`
We are aware that an `R CMD check` command on this package will give a **NOTE on extdata folder size bigger than 1MB**. However, as we want to showcase the complete process followed through this project, we include the processing of these bigger raw files in our package. The data selection and customization made for the analysis and discussion subsections is done in the *Data inspection* section.

## II.1 Madrid pollution data processing

We begin with a custom arrangement of our data files:

```{r}
data_path <- system.file("extdata/Year2023/", package="widepollution")
file_paths <- list.files(data_path, full.names = TRUE, pattern = "*.csv")

#We extract a list of months ordered as the files are
file_names <- list.files(data_path, full.names = FALSE, pattern = "*.csv")
months <- lapply(file_names,function(str){
  month <- strsplit(str,'_',fixed=TRUE)[[1]][1]
}) |> unlist()

#Reordering  of files in a custom (chronological) order from the Spanish names
files <- data.frame(path=file_paths, month=months)
months_order <- c('ene','feb','mar','abr','may','jun','jul','ago','sep','oct','nov','dic')
files <- arrange(files,factor(files$month,levels = months_order))
head(files,5)
```

We iterate data extraction and an unpivot process through all the monthly files. Then we construct a single long-format dataframe. We perform processing to ensure appropriate data types and set a useful dataframe structure for further analyses.

```{r}
monthly_data <- list()

#Extraction and unpivot
for (month in 1:12){
  month_path <- files$path[month]
  df <- read.csv(month_path,sep=';',fileEncoding = "UTF-8")
  monthly_data[[month]] <- parallel_unpivot(df,
                                            'H',
                                            'V',
                                            'measure',
                                            'validation',
                                            'hour',
                                            '(H|V)([[:digit:]]+)')
}

year_data <-  do.call('rbind',monthly_data)

#Change data type of extracted hour column
year_data$hour <- as.integer(year_data$hour)

year_data$measure <- as.numeric(year_data$measure)

#Rename and extract columns of interest
year_data  <- rename_columns(year_data,
                           c('ESTACION','MES','DIA','MAGNITUD'),
                           c('station_id','month','day','pollutant_id'))
year_data <- year_data[c('station_id','pollutant_id','month','day','hour','validation','measure')]

#Convert V-F values to booleans for the validation column
year_data$validation <- lapply(year_data$validation, function(x){x=='V'})|>unlist()


head(year_data)
```

We incorporate pollutants metadata with descriptive information.

```{r}
pollutants <- system.file('extdata/pollutants.csv',package = 'widepollution')|> read.csv(sep=',')
pollutants <- rename_columns(pollutants,
                             c('Cod.','Abrevia.','Unidad'),
                             c('pollutant_id','pollutant_name','unit')
                             )
pollutants <- pollutants[c('pollutant_id','pollutant_name','unit')]

pollutants$unit <- gsub('m3','m^3',pollutants$unit) 

pollutants$unit|>enc2utf8()-> pollutants$unit #Ensuring utf-8

year_data <- merge(year_data,pollutants,by='pollutant_id')
head(year_data)
```

### Dataset inspection

We can now get a glance of the proportion of "verified" samples from the dataset, following the metadata description available on the portal:

```{r}
verified <- sum(year_data$validation)
total <- dim(year_data)[1]
paste0(round(verified/total * 100,2),' %')
```

We verify that no `NULL` values are provided in the dataset:

```{r}
paste0('Number of NULL measures found: ',
       sum(unlist(lapply(year_data$measure,is.null))))
```

We now inspect the proportion of data by type of pollutant and compare it with the desired number of data measurements, namely `number of stations`$\times\ 365$ `days of the year` $\times\ 24$ `hours of the day`.

```{r}
pollutant_summary <- aggregate(measure~pollutant_name,data= year_data,length)|>
  rename_columns('measure','num_of_measures')
pollutant_summary <- pollutant_summary[order(pollutant_summary$num_of_measures,decreasing=TRUE),]


desired_num_measurements <- length(unique(year_data$station_id)) * 365 * 24
pollutant_summary$percentage_of_completeness <- pollutant_summary$num_of_measures*100 /desired_num_measurements

pollutant_summary
```

Though no `NULL` values were found in the dataset, it seems that **most pollutants lack data** through time and across the sensor network. This is why the interpolation methods are leveraged to **account for missing data** around a particular sensor in a given time.

For simplicity and leveraging the most complete dataset, we will do our analysis with the $NO_x$. We thus inspect in more detail data from this pollutant. We should have, by station-month combination, a total of measurements equal to the `number of days of that month` $\times\ 24$ `hours of the day`. We know, by the wide structure of our initial dataset, that missing hours will not be the case, so we perform the inspection at the "day level" (division by $24$):

```{r}
#Extract NOx pollution data
nox_madrid <- year_data[year_data$pollutant_name=='NOx',]

#Compute a list of desired values per month (i.e. january = 31, february = 28, etc.)
desired_vals <- difftime(
  as.Date(paste0('2023-',1:12,'-01')),
c(as.Date(paste0('2023-',2:12,'-01')),as.Date('2024-01-01'))) |> abs() |> as.numeric() 

#Retrieve a wide-format summary of the number of monthly missing measurements per station
aggregate(measure~month+station_id,nox_madrid,function(x){length(x)/24})|>
  pivot_wider(
    names_from = station_id,
    names_glue = "station_{station_id}",
    values_from = measure,
    values_fill = 0) -> nox_count_summary

cols <- 2:length(nox_count_summary)
nox_count_summary[cols] <- lapply(nox_count_summary[cols],function(x){x-desired_vals})
nox_count_summary
```

Inspecting this summary dataframe, we conclude that most stations possess complete $NO_x$ records per month and, most important, missing dates spatially concentrate in few stations. Therefore, we can get proxy readings for those locations by interpolation. With this in mind, the $NO_x$ dataset has been stored as data in our package for later use.

## II.2 Integration of stations locations

We now load the stations data and locations (point geometries), defining the column `station_id` to perform the dataframes join when needed.

```{r}
#Read stations csv
file_path <- system.file('extdata/stations.csv',package = 'widepollution')
stations <- read.csv(file_path,sep=';',fileEncoding = "UTF-8")

#Translate and align column names
stations <- rename_columns(stations,c('CODIGO_CORTO','ESTACION','LONGITUD','LATITUD'),c('station_id','station_name','long','lat'))
stations <- stations[c('station_id','station_name','long','lat')]

#Ensuring ASCII characters for names (key step to avoid warnings/notes on package)
stations$station_name|>iconv(,to='ASCII//TRANSLIT') -> stations$station_name

#Set georeference
stations_crs <- st_crs("WGS84" )
stations <- st_as_sf(stations,coords=c('long','lat'),crs=stations_crs)
stations
```

We then join the dataframes and inspect the size of the dataset (in MBs).

```{r}
#Data join
nox_madrid <- merge(nox_madrid,stations,by='station_id')|>
  st_sf(crs = stations_crs)

#Inspection of NOx dataset size (with geometries)
(object.size(nox_madrid)/10e6) |> 
  as.numeric()|>
  set_units(,value=MB)

```

## II.3 Neighborhoods data processing

To cexhibit the handling of another data type (JSON file) the following steps integrate the Madrid neighborhoods to our workflow to provide more spatial context and extract valuable insights per neighborhood. The original dataset provides the neighborhoods' border linestring coordinates given in the `ETRS89` CRS, which has `meters` as units. This will be suitable in the furthcoming analyses. However, for consistency with the pollution dataset, now we reproject to `WGS84`.

```{r}
#Extraction of data from json file
file_path <- system.file('extdata/neighborhoods.json',package='widepollution')
read <- paste(readLines(file_path, warn=FALSE),collapse = "")
neighborhoods_json <- fromJSON(read)
neighborhoods_crs <- st_crs(neighborhoods_json$crs$properties$name)

#Extraction of sf linestring objects from json using helping function
num_neighborhoods <- length(neighborhoods_json$features)
neighborhoods <- data.frame(FID = 1:num_neighborhoods)
neighborhoods$geometry <- lapply(neighborhoods$FID,function(x){extract_linestring_from_json_list(x,neighborhoods_json)})

#Final construction of sf collection
neighborhoods <- st_as_sf(neighborhoods,crs = neighborhoods_crs)|>
  st_polygonize()|>
  st_transform(,crs=stations_crs)

#Incorporation of metadata and selection of meaningful columns
neigh_metadata <- system.file('extdata/neighborhoods_metadata.csv',package='widepollution')|> read.csv(sep = ',',fileEncoding = "UTF-8")
neigh_metadata$NOMBRE|>iconv(,to='ASCII//TRANSLIT') -> neigh_metadata$NOMBRE #Ensuring ASCII characters

neighborhoods <- merge(neighborhoods,neigh_metadata,by = 'FID')
neighborhoods <- rename_columns(neighborhoods,c('NOMBRE','NOMDIS'),c('name','district'))
neighborhoods <- neighborhoods[c('FID','name','district')]
```

First rendering of a plot for contextual and consistency verification:

```{r}
#Plot of pollution stations and neighborhoods
par(mar = c(2,2,0.5,0.5), mgp = c(0, 1, 0))
plot(neighborhoods$geometry,graticule=TRUE,col_graticule='gray',lwd = 0.1,col='lightblue',axes=TRUE,border=0)
plot(stations$geometry,lwd=2,col='orange',add=TRUE)
```

Moreover, we make sure that the most persistently missing data (in stations with `station_id` 11 and 35, shown in the `Dataset inspection` section) is enclosed by nearby sensors:

```{r}
par(mar = c(2,2,0.5,0.5), mgp = c(0, 1, 0))
plot(neighborhoods$geometry,graticule=TRUE,col_graticule='gray',lwd = 0.1,col='lightblue',axes=TRUE,border=0)
plot(stations$geometry,add=TRUE)
plot(stations$geometry[stations$station_id%in%c(11,35)],col='red',add=TRUE,lwd = 2)
```

# III. Analysis

In this section we perform the data analysis of the previously retrieved, transformed and constructed datasets. To showcase the possibility of our package to store datasets as R objects, we consume the $NO_x$ dataset by the `package::dataset` call:

```{r}
nox_madrid <- widepollution::nox_madrid
```

## III.1 Monthly data analysis

We execute data aggregation at the month level and perform interpolation through `Inverse Distsance Interpolation` per month.

```{r}
#Month-station data aggregation with mean values
nox_mad_monthly <- aggregate(measure~month+station_id,nox_madrid,mean)|>
  merge(stations,by='station_id')|>
  st_sf(crs=stations_crs)|>
  st_transform(crs=neighborhoods_crs) #transforming to ETRS89 / UTM30 to gain support for meters as unit

#Obtain the whole Madrid area to clip grid
neighborhoods$geometry |>
  st_union() |> st_transform(crs=neighborhoods_crs) -> madrid

grid <- madrid |> st_bbox() |>
  st_as_stars(dx=100) |> #100m resolution to ease calculations
  st_crop(madrid)

#Perform interpolation per month and construct a stars data cube

l <- list()

for (num_month in 1:12){
  month_data <- nox_mad_monthly[nox_mad_monthly$month==num_month,]
  inter <- idw(measure~1, month_data, grid)
  names(inter)[[1]]<-months_order[num_month] #naming of monthly variables with a month (Spanish) label
  l[[num_month]] <- inter [1,,]
}


nox_mad_monthly_cube <- do.call(c,l)|>merge(name = "month") |> setNames("NOx concentration")
nox_mad_monthly_cube
```

### Neighborhoods discussion

We now take advantage of the `stars` object created to perform an analysis per neighborhood that can be useful for the authorities, in order to identify risk at that level. Explicitly, we compute the mean concentration of $NO_x$ per neighborhood throughout the year.

```{r}
aggregator <- st_transform(neighborhoods$geometry,crs=neighborhoods_crs)
nox_neigh_month <- aggregate(nox_mad_monthly_cube,aggregator,mean)
names(nox_neigh_month) <- 'mean NOx concentration'
nox_neigh_month
```

We inspect the evolution of concentration of $NO_x$ in the first half of the year by performing an adequate (vector) data cube subsetting:

```{r}
nox_neigh_month[,,1:6] |> plot(breaks='jenks')
```

We readily verify that the beginning of the year suffered of most critical levels of $NO_x$ pollution and that this pollutant tends to concentrate in the center to center-south corridor of the city, verifying what was commented in the `Introduction`. Morevoer, we can obtain time statistics per neighborhood, such as a broad yearly figure (mean value) of the $NO_x$ concentration:

```{r}
#Summarize by geometry
st_apply(nox_neigh_month,c('geometry'),mean) |> as.data.frame() |> st_sf() -> nox_mad_mean_year

#Retrieving names of neighborhoods
nox_mad_mean_year$FID <- 1:131
nox_mad_mean_year <- merge(nox_mad_mean_year,
      st_drop_geometry(neighborhoods[c('FID','name')]),
      by='FID')

plot(nox_mad_mean_year['mean'],main = 'Mean NOx concentration in Madrid (2023)',breaks='jenks')
```

We could recover the neighborhoods names by the id, as the aggregation step was done following the order of the `neighborhoods` sf object. Then, we can obtain the 10 most polluted neighborhoods by name and plot them as well:

```{r}
most_polluted <- nox_mad_mean_year[order(nox_mad_mean_year$mean,decreasing = TRUE),][1:10,c('name','mean')]
most_polluted$name
```

```{r}
#Plot the most polluted neighborhoods with greater mean concentration of NOx through 2023
neighborhoods$geometry|>st_transform(crs=neighborhoods_crs)|>
  plot(border=0,col='lightblue',graticule=TRUE,axes=TRUE)
most_polluted$geometry |> st_union() |> st_buffer(1) |>#mini buffer to clean unprecise union
  plot(lwd = 2,border ='red', alpha = 0,add=TRUE)
```

## III.2 Hour data analysis

Now we perform a similar analysis, this time first aggregating the pollution data into hour-of-the-day slots across the year:

```{r}
#Hour of day-station data aggregation with mean values
nox_mad_hour <- aggregate(measure~hour+station_id,nox_madrid,mean)|>
  merge(stations,by='station_id')|>
  st_sf(crs=stations_crs)|>
  st_transform(crs=neighborhoods_crs) #transforming to ETRS89 / UTM30 to gain support for meters as unit


#Perform interpolation per hour-of-the-day and construct a stars data cube

l <- list()

for (hour in 1:24){
  month_data <- nox_mad_hour[nox_mad_hour$hour==hour,]
  capture.output(invisible(inter <- idw(measure~1, month_data, grid)))#avoid printing each interpolation stage
  names(inter)[[1]]<-hour
  l[[hour]] <- inter [1,,]
}

nox_mad_hour_cube <- do.call(c,l)|>merge(name = "hour") |> setNames("NOx concentration")
nox_mad_hour_cube
```

### Neighborhoods discussion

We obtain the hour-of-the-day $NO_x$ concentration figure by neighborhood for further discussion.

```{r}
aggregator <- st_transform(neighborhoods$geometry,crs=neighborhoods_crs)
nox_neigh_hour <- aggregate(nox_mad_hour_cube,aggregator,mean)
names(nox_neigh_hour) <- 'mean NOx concentration'
nox_neigh_hour
```

We inspect the evolution of concentration of $NO_x$ along, for instance, an even afternoon of 2023 (from 13 to 20 hrs.) in the metropolitan area of Madrid:

```{r}
nox_neigh_hour[,,13:20] |> plot(breaks='jenks')
```

We clearly identify the rush hour pollution rise after 19:00 (when vehicles are heavily used). It represents a peak time slot where major concentration around the city can be found and, in this case, the patterns builds up increasingly towards the outskirts of the city. It is interesting to identify two nuclei at the south and north of the city center building up as the afternoon passes, until an overall high level (over 50 $\mu g/m^3$ of $NO_x$ concentration) is registered around the center after 20:00.

It is also interesting to notice that during working hours the major concentration develops at the center of the city. Moreover, we can identify a great dispersion of $NO_x$ concentration values during the peak hours in the following graph:

```{r}
nox_neigh_hour |> as.data.frame() |> st_drop_geometry() |> rename_columns(c('mean.NOx.concentration'),c('NOx concentration')) -> nox_neigh_hour_df
nox_neigh_hour_df$FID <- 1:131

nox_neigh_hour_df <- merge(nox_neigh_hour_df[c('FID','NOx concentration','hour')],
      st_drop_geometry(neighborhoods[c('FID','name')]),
      by='FID')

#Plot time series values
nox_unit <-paste0('[',pollutants[pollutants$pollutant_name=='NOx','unit'],']')
df <- nox_neigh_hour_df[c('hour','name','NOx concentration')]
plot(df$hour,df$`NOx concentration`,
     main = 'NOx mean concentration accross the day (2023)',
     xlab='Hour of the day',
     ylab=nox_unit
     )
```

We now take a closer look to the time series of the 10 overall most polluted neighborhoods. We verify that these neighborhoods show $NO_x$ concentration values (~80 $\mu g/m^3$) similar to those of the upper part of the value intervals of the previous graph during the peak hours (around 9:00 and after 19:00):

```{r}
most_polluted_hour <- nox_neigh_hour_df[nox_neigh_hour_df$name %in% most_polluted$name,]
most_polluted_hour <- most_polluted_hour[order(most_polluted_hour$hour),]


df <- most_polluted_hour[c('hour','name','NOx concentration')]
plot(df$hour,df$`NOx concentration`,
     main = 'NOx mean concentration accross the day (2023)',
     xlab='Hour of the day',
     ylab=nox_unit
     )
```

Therefore, we conclude that these 10 polluted neighborhoods not only show a strong overall $NO_x$ concentration during 2023, but also show tougher increases around the rush hours. Explicitly, for these 10 neighborhoods located in the centre-south of the city, the average $NO_x$ concentration from 19:00 to 21:00 is:

```{r}
val <- mean(most_polluted_hour[most_polluted_hour$hour%in%19:21,]$`NOx concentration`) |> round(2)
cat(
  paste0('NOx 2023 mean concentration of most  polluted neighborhoods from 19 to 21 hours:\n   ',val," ",nox_unit),
  paste0('\n\nPercentage above recommendation:\n   ',
       (val/40-1)*100,'%')  
)
```

which is far above (78.7%) the 40 $\mu g/ m^3$ recommendation by the EEA, as commented in the `Introduction`.

# References

Borge, R., Lumbreras, J., Pérez, J., de la Paz, D., Vedrenne, M., de Andrés, J. M., & Rodríguez, M. E. (2014). Emission inventories and modeling requirements for the development of air quality plans. Application to Madrid (Spain). The Science of the total environment, 466-467, 809–819. <https://doi.org/10.1016/j.scitotenv.2013.07.093>

European Environment Agency. (2019). “Air quality in Europe—2019 report”. EEA Report No 10/2019.Retrieved on June 24 at <https://www.eea.europa.eu/publications/air-quality-in-europe-2019>

Ortiz, C., Linares, C., Carmona, R., & Díaz, J. (2017). Evaluation of short-term mortality attributable to particulate matter pollution in Spain. Environmental pollution (Barking, Essex : 1987), 224, 541–551. <https://doi.org/10.1016/j.envpol.2017.02.037>
